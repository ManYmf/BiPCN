{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m run_experiment(\n\u001b[32m     10\u001b[39m     user_name=\u001b[33m\"\u001b[39m\u001b[33mmengfan\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     experiment_name=\u001b[33m\"\u001b[39m\u001b[33må…³äºŽæ¯”ä¾‹çš„æœ€ç»ˆå®žéªŒ\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     param_space=\u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     13\u001b[39m         seed = tune.grid_search([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m]),  \n\u001b[32m     14\u001b[39m         alpha_gen = tune.grid_search([\u001b[38;5;28mround\u001b[39m(x, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m np.arange(\u001b[32m0.1\u001b[39m, \u001b[32m1.01\u001b[39m, \u001b[32m0.1\u001b[39m)]),\n\u001b[32m     15\u001b[39m         alpha_disc = tune.grid_search([\u001b[38;5;28mround\u001b[39m(x, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m np.arange(\u001b[32m0.1\u001b[39m, \u001b[32m1.01\u001b[39m, \u001b[32m0.1\u001b[39m)]),\n\u001b[32m     16\u001b[39m         epochs = tune.grid_search([\u001b[32m25\u001b[39m]),\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m     ),\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# IMPORTANT: when using cluster, your trainable cannot be defined in this script, but has to be defined in a separate file, and imported in this script\u001b[39;00m\n\u001b[32m     20\u001b[39m     trainable=trainable,\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# When use cluster, local_storage_path needs to be specified in this way\u001b[39;00m\n\u001b[32m     22\u001b[39m     local_storage_path=get_ray_results_dir(),\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Specify the cluster to use\u001b[39;00m\n\u001b[32m     24\u001b[39m     run_with=\u001b[33m\"\u001b[39m\u001b[33mcluster:atol-gpu-5090\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Specify gram per trial\u001b[39;00m\n\u001b[32m     26\u001b[39m     cpu_per_trial=\u001b[32m1\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# If the gram_per_trial you set is larger than GRAM of a single GPU, you will be allocated with more than one GPUs\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# to fit your need. But in this case, you will need to handle how do you want to distribute your workload to these multiple GPUs\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# To check GRAM of a single GPU, use the following code:\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# from radas.clusters import gram_per_gpu\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     32\u001b[39m     runtime_env={\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33memoji\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mscikit-learn\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     34\u001b[39m         \u001b[38;5;66;03m# for more runtime_env specifications, see https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference\u001b[39;00m\n\u001b[32m     35\u001b[39m     },\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     37\u001b[39m     instant_gpu_allocation=\u001b[32m1\u001b[39m,\n\u001b[32m     38\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/radas/radas/run_experiment.py:471\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(user_name, trainable, experiment_name, ssh_key_path, cpu_per_trial, run_with, cloud_storage_path, local_storage_path, runtime_env, use_proxy, tuner_init_kwargs, tuner_restore_kwargs, num_seeds, param_space, instant_gpu_allocation, tuner_to_df_fn, process_df_fn, process_df_fn_kwargs, process_sns_fn, plot_fn, plot_kwargs, rename_mapping, matplotlib_rcParams_update, process_g_fn, plot_format, dos, yes)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == \u001b[33m\"\u001b[39m\u001b[33mtail\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    470\u001b[39m     print_info(\u001b[33m\"\u001b[39m\u001b[33mTailing experiment logs below ðŸ‘‡ðŸ‘‡ðŸ‘‡\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     tail_logs_return_info = \u001b[38;5;28;01mawait\u001b[39;00m job.tail_logs()\n\u001b[32m    472\u001b[39m     print_info(\u001b[33m\"\u001b[39m\u001b[33mTailing experiment logs ended ðŸ‘†ðŸ‘†ðŸ‘†\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    473\u001b[39m     job_status = job.get_job_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/radas/radas/clusters/ftjob.py:72\u001b[39m, in \u001b[36mFtJob.tail_logs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     69\u001b[39m buffer = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m buffer_size = \u001b[32m8192\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m lines \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.tail_job_logs(\n\u001b[32m     73\u001b[39m     experiment_name=\u001b[38;5;28mself\u001b[39m.meta_config[\u001b[33m\"\u001b[39m\u001b[33mexperiment_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     74\u001b[39m     time_stamp=\u001b[38;5;28mself\u001b[39m.meta_config[\u001b[33m\"\u001b[39m\u001b[33mtime_stamp\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     75\u001b[39m ):\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m(lines, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m     buffer += lines\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/radas/lib/python3.11/site-packages/ray/dashboard/modules/job/sdk.py:497\u001b[39m, in \u001b[36mJobSubmissionClient.tail_job_logs\u001b[39m\u001b[34m(self, job_id)\u001b[39m\n\u001b[32m    492\u001b[39m ws = \u001b[38;5;28;01mawait\u001b[39;00m session.ws_connect(\n\u001b[32m    493\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._address\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/jobs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/logs/tail\u001b[39m\u001b[33m\"\u001b[39m, ssl=\u001b[38;5;28mself\u001b[39m._ssl_context\n\u001b[32m    494\u001b[39m )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     msg = \u001b[38;5;28;01mawait\u001b[39;00m ws.receive()\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg.type == aiohttp.WSMsgType.TEXT:\n\u001b[32m    500\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m msg.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/radas/lib/python3.11/site-packages/aiohttp/client_ws.py:336\u001b[39m, in \u001b[36mClientWebSocketResponse.receive\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    334\u001b[39m             msg = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reader.read()\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m         msg = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reader.read()\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset_heartbeat()\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/radas/lib/python3.11/site-packages/aiohttp/_websocket/reader_c.py:118\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/radas/lib/python3.11/site-packages/aiohttp/_websocket/reader_c.py:115\u001b[39m, in \u001b[36maiohttp._websocket.reader_c.WebSocketDataQueue.read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from radas import run_experiment, get_ray_results_dir\n",
    "import ray.tune as tune\n",
    "from v10_1 import trainable\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "results = await run_experiment(\n",
    "    user_name=\"mengfan\",\n",
    "    experiment_name=\"å…³äºŽæ¯”ä¾‹çš„æœ€ç»ˆå®žéªŒ\",\n",
    "    param_space=dict(\n",
    "        seed = tune.grid_search([0, 1]),  \n",
    "        alpha_gen = tune.grid_search([round(x, 1) for x in np.arange(0.1, 1.01, 0.1)]),\n",
    "        alpha_disc = tune.grid_search([round(x, 1) for x in np.arange(0.1, 1.01, 0.1)]),\n",
    "        epochs = tune.grid_search([25]),\n",
    "\n",
    "    ),\n",
    "    # IMPORTANT: when using cluster, your trainable cannot be defined in this script, but has to be defined in a separate file, and imported in this script\n",
    "    trainable=trainable,\n",
    "    # When use cluster, local_storage_path needs to be specified in this way\n",
    "    local_storage_path=get_ray_results_dir(),\n",
    "    # Specify the cluster to use\n",
    "    run_with=\"cluster:atol-gpu-5090\",\n",
    "    # Specify gram per trial\n",
    "    cpu_per_trial=1,\n",
    "    # If the gram_per_trial you set is larger than GRAM of a single GPU, you will be allocated with more than one GPUs\n",
    "    # to fit your need. But in this case, you will need to handle how do you want to distribute your workload to these multiple GPUs\n",
    "    # To check GRAM of a single GPU, use the following code:\n",
    "    # from radas.clusters import gram_per_gpu\n",
    "    #\n",
    "    runtime_env={\n",
    "        \"pip\": [\"emoji\",\"scikit-learn\"],\n",
    "        # for more runtime_env specifications, see https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference\n",
    "    },\n",
    "    #\n",
    "    instant_gpu_allocation=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"df\"]\n",
    "df = results[\"df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_3D import plot_alpha_2d_contours\n",
    "plot_alpha_2d_contours(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
